# Model configuration
# Path to pretrained model or model identifier from huggingface.co/models
model_name_or_path: /mnt/user-ssd/lilei35/oss_models/Qwen2.5-VL-7B-Instruct
# Attention implementation: "sdpa" or "flash_attention_2"
attn_implementation: flash_attention_2
# Allow custom model code execution
# Use bfloat16 precision
# Image processing parameters
min_pixels: 6272  # 8 * 28 * 28
max_pixels: 1605632  # 4096 * 28 * 28
dataloader_num_workers: 4 
dataloader_prefetch_factor: 2 
# Data processing configuration
# Path to the dataset
dataset_path: /mnt/user-ssd/lilei35/trl/balance_100_en_zh_total30k
# Maximum sequence length for tokenization
max_seq_length: 10240 
# Optional: limit samples for debugging
max_train_samples: null
max_eval_samples: null

# Training configuration
# Output directory for checkpoints and logs
output_dir: ./vlm_reward_model_balance_100_en_zh_total30k 
# Training hyperparameters
max_steps: 3000  # 
per_device_train_batch_size: 1 
per_device_eval_batch_size: 2 
gradient_accumulation_steps: 4
learning_rate: 1.0e-5
# Training loop settings
eval_steps: 200
save_steps: 100
warmup_steps: 100
logging_steps: 10
# Optimization settings
gradient_checkpointing: true
disable_dropout: true
# DeepSpeed configuration file
deepspeed: zero2.json 
bf16: true 
