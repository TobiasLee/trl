# Model configuration
# Path to pretrained model or model identifier from huggingface.co/models
model_name_or_path: /mnt/bos-multimodal/songzc/model/XiaomiMiMo/MiMo-VL-7B-RL
#/mnt/user-ssd/lilei35/oss_models/Qwen2.5-VL-7B-Instruct 
#/mnt/user-ssd/lilei35/oss_models/Qwen2.5-VL-7B-Instruct
# Attention implementation: "sdpa" or "flash_attention_2"
attn_implementation: flash_attention_2
# Allow custom model code execution
# Use bfloat16 precision
# Image processing parameters
min_pixels: 6272  # 8 * 28 * 28
max_pixels: 1605632  # 4096 * 28 * 28
dataloader_num_workers: 4 
dataloader_prefetch_factor: 2 
# Data processing configuration
# Path to the dataset
dataset_path: /mnt/ssd-qinghai/lilei35/balance_100_en_zh_total30k
# Maximum sequence length for tokenization
max_seq_length: 10280   
# Optional: limit samples for debugging
max_train_samples: null
max_eval_samples: null

# irrelevant query augmentation 
use_irrelevant_queries: true
  # false
num_irrelevant_pairs: 1 
irrelevant_loss_weight: 1.0  
freeze_vit: true  
# Training configuration
# Output directory for checkpoints and logs
output_dir: /mnt/ssd-qinghai/lilei35/vlm_rms/mimo7b_rl_augment_reward_model_balance_100_en_zh_total30k_freezeViT_1e-5_aug1
# Training hyperparameters
max_steps: 1500   # 
per_device_train_batch_size: 2 
per_device_eval_batch_size: 2 
gradient_accumulation_steps: 4
learning_rate: 1.0e-5
# Training loop settings
eval_steps: 100
save_steps: 100
warmup_steps: 100
logging_steps: 10
# Optimization settings
gradient_checkpointing: true
disable_dropout: true
# DeepSpeed configuration file
deepspeed: zero3.json 
bf16: true 
